{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Using Reddit's API for Classifying Subreddit Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-23T19:28:02.619411Z",
     "start_time": "2017-10-23T19:28:02.600856Z"
    }
   },
   "source": [
    "In this project, we will practice two major skills. Collecting data via an API request and then building a binary predictor.\n",
    "\n",
    "As we discussed in week 2, and earlier today, there are two components to starting a data science problem: the problem statement, and acquiring the data.\n",
    "\n",
    "For this article, your problem statement will be: _What characteristics of a post on Reddit contribute most to what subreddit it belongs to?_\n",
    "\n",
    "Your method for acquiring the data will be scraping threads from at least two subreddits. \n",
    "\n",
    "Once you've got the data, you will build a classification model that, using Natural Language Processing and any other relevant features, predicts which subreddit a given post belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "### Scraping Thread Info from Reddit.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a request (using requests) to the URL below. \n",
    "\n",
    "*NOTE*: Reddit will throw a [429 error](https://httpstatuses.com/429) when using the following code:\n",
    "```python\n",
    "res = requests.get(URL)\n",
    "```\n",
    "\n",
    "This is because Reddit has throttled python's default user agent. You'll need to set a custom `User-agent` to get your request to work.\n",
    "```python\n",
    "res = requests.get(URL, headers={'User-agent': 'YOUR NAME Bot 0.1'})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "\n",
    "# For webscraping\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Regular data cleaning/handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geocaching subreddit\n",
    "URL = \"https://www.reddit.com/r/geocaching.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save requests as an object using .get\n",
    "res = requests.get(URL, headers = {'User-agent': 'nicoleBot'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `res.json()` to convert the response into a dictionary format and set this to a variable. \n",
    "\n",
    "```python\n",
    "data = res.json()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a json\n",
    "data = res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Post body text\n",
    "data['data']['children'][1]['data']['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A quick Virtual Cache for the August Geochallenge!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Post title\n",
    "data['data']['children'][1]['data']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "# How many posts in the json\n",
    "print(len(data['data']['children']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t3_99g4sx'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the 'after' value?\n",
    "data['data']['after']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting more results\n",
    "\n",
    "By default, Reddit will give you the top 25 posts:\n",
    "\n",
    "```python\n",
    "print(len(data['data']['children']))\n",
    "```\n",
    "\n",
    "If you want more, you'll need to do two things:\n",
    "1. Get the name of the last post: `data['data']['after']`\n",
    "2. Use that name to hit the following url: `http://www.reddit.com/r/boardgames.json?after=THE_AFTER_FROM_STEP_1`\n",
    "3. Create a loop to repeat steps 1 and 2 until you have a sufficient number of posts. \n",
    "\n",
    "*NOTE*: Reddit will limit the number of requests per second you're allowed to make. When you create your loop, be sure to add the following after each iteration.\n",
    "\n",
    "```python\n",
    "time.sleep(3) # sleeps 3 seconds before continuing```\n",
    "\n",
    "This will throttle your loop and keep you within Reddit's guidelines. You'll need to import the `time` library for this to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What do you think of field puzzles that are solved simply with lots of trial and error?'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring how to get the next 25 posts\n",
    "\n",
    "new_url = 'http://www.reddit.com/r/geocaching.json?after=t3_99c6t7'\n",
    "res = requests.get(new_url, headers={'User-agent': 'nicoleBot'})\n",
    "new_data = res.json()\n",
    "new_data['data']['children'][0]['data']['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geocaching posts captured, don't run this twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Capturing Geocaching posts\n",
    "# url = 'http://www.reddit.com/r/geocaching.json'\n",
    "\n",
    "# all_posts = []\n",
    "\n",
    "# for _ in range(40):\n",
    "#     #construct list of 1000 posts\n",
    "    \n",
    "#     # Get posts by going to the url, put it into json, and store it\n",
    "#     res = requests.get(url, headers={'User-agent': 'nicoleBot'})\n",
    "#     data = res.json()\n",
    "    \n",
    "#     # save only the posts out of the json into the list_of_posts, then add\n",
    "#     # add all the posts to the all_posts list\n",
    "#     list_of_posts = data['data']['children']\n",
    "    \n",
    "    \n",
    "#     # do your data cleaning every time you pull, then only\n",
    "#     # save the info that you need\n",
    "#     for post in list_of_posts:\n",
    "#         current_row = []\n",
    "#         current_row.append(post['data']['selftext'])\n",
    "#         current_row.append(post['data']['title'])\n",
    "#         current_row.append(post['data']['subreddit'])  \n",
    "#         all_posts.append(current_row)\n",
    "        \n",
    "    \n",
    "#     # reassign the after to the current 'after', and then update the url to hit\n",
    "#     after = data['data']['after']\n",
    "#     url = 'http://www.reddit.com/r/geocaching.json?after=' + after\n",
    "    \n",
    "#     # go to sleep for 3 seconds so you do not overwhelm reddit and get kicked out\n",
    "#     print('The current after: ', after)\n",
    "#     time.sleep(3)\n",
    "\n",
    "# now put the list of lists, where which inner list is a row\n",
    "# straight into a dataframe\n",
    "# df = pd.DataFrame(all_posts, columns = ['text','title','subreddit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webscraping a second subreddit\n",
    "\n",
    "I chose the subreddit I Want To Learn, as I feel it would have a wider range of vocabulary and subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_url = 'http://www.reddit.com/r/IWantToLearn.json'\n",
    "res = requests.get(learn_url, headers={'User-agent': 'nicoleBot'})\n",
    "learn_data = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IWantToLearn'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm what subreddit we've scraped from and how to navigate down through the json\n",
    "learn_data['data']['children'][0]['data']['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IWTL posts captured, do not run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Repeat process for IWantToLearn subbreddit\n",
    "\n",
    "# url = 'http://www.reddit.com/r/IWantToLearn.json'\n",
    "\n",
    "# all_posts = []\n",
    "\n",
    "# for _ in range(3):\n",
    "#     #construct list of 1000 posts\n",
    "    \n",
    "#     # Get posts by going to the url, put it into json, and store it\n",
    "#     res = requests.get(url, headers={'User-agent': 'nicoleBot'})\n",
    "#     data = res.json()\n",
    "    \n",
    "#     # save only the posts out of the json into the list_of_posts, then add\n",
    "#     # add all the posts to the all_posts list\n",
    "#     list_of_posts = data['data']['children']\n",
    "    \n",
    "    \n",
    "#     # do your data cleaning every time you pull, then only\n",
    "#     # save the info that you need\n",
    "#     for post in list_of_posts:\n",
    "#         current_row = []\n",
    "#         current_row.append(post['data']['selftext'])\n",
    "#         current_row.append(post['data']['title'])\n",
    "#         current_row.append(post['data']['subreddit'])\n",
    "#         all_posts.append(current_row)\n",
    "        \n",
    "    \n",
    "#     # reassign the after to the current 'after', and then update the url to hit\n",
    "#     after = data['data']['after']\n",
    "#     url = 'http://www.reddit.com/r/IWantToLearn.json?after=' + after\n",
    "    \n",
    "#     # go to sleep for 3 seconds so you do not overwhelm reddit and get kicked out\n",
    "#     print('The current after: ', after)\n",
    "#     time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dataframe\n",
    "learn_df = pd.DataFrame(all_posts, columns = ['text','title','subreddit'])\n",
    "\n",
    "# Concatenate onto geocaching df for a full dataset\n",
    "two_subr = pd.concat([df, learn_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Save your results as a CSV\n",
    "You may do this regularly while scraping data as well, so that if your scraper stops of your computer crashes, you don't lose all your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_subr.to_csv('reddit_posts.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
