{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Using Reddit's API for Classifying Subreddit Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook regards the classification of geocaching and IWantToLearn subreddit posts from Reddit using Natural Language Processing techniques. For the workflow of obtaining the posts using webscraping techniques, please refer to the Jupyter Notebook \"Project 3 Webscraping.ipynb\". This project was segmented into two to prevent from re-running webscraping code.\n",
    "\n",
    "\n",
    "\n",
    "Problem statement: What characteristics of a post on Reddit contribute most to what subreddit it belongs to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning/handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# NLP specific libraries\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "import re\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Modeling Libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring in Dataset of Posts scraped from Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./reddit_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geocaching      1000\n",
       "IWantToLearn    1000\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are an even 1000 posts scraped from Reddit from our Webscraping efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using regular expressions to remove punctuation in a function\n",
    "def no_punct(string):\n",
    "    return re.sub(\"[.,😯?😊!’\\\";^+`:*'()-@”“=>_$&<~%|{}\\[\\]]\", \" \", string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to clean any column or to feed into a word vectorizer as an analyzer parameter\n",
    "def clean_func(column):\n",
    "    \n",
    "    #remove puntuation with punctuation removal function\n",
    "    column = no_punct(column)\n",
    "    \n",
    "    #lowercase\n",
    "    column = column.lower()\n",
    "    \n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that utilizes lemmatizing and a general Regex to remove punctuation\n",
    "\n",
    "def preprocess(text):\n",
    "    # instantiate lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # words only Regex, removes punctuation\n",
    "    text = re.sub(\"[^A-Za-z]\", \" \", text)\n",
    "    \n",
    "    # lemmatize\n",
    "    text = lemmatizer.lemmatize(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1995, 3)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>1364</td>\n",
       "      <td>1364</td>\n",
       "      <td>Especially in piano, basketball, etc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>1995</td>\n",
       "      <td>1989</td>\n",
       "      <td>I want to learn to speak intellectually.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <td>1995</td>\n",
       "      <td>2</td>\n",
       "      <td>geocaching</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count unique                                       top freq\n",
       "text       1364   1364      Especially in piano, basketball, etc    1\n",
       "title      1995   1989  I want to learn to speak intellectually.    2\n",
       "subreddit  1995      2                                geocaching  999"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>IWantToLearn</th>\n",
       "      <th>geocaching</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">text</th>\n",
       "      <th>count</th>\n",
       "      <td>890</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>890</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Especially in piano, basketball, etc</td>\n",
       "      <td>Would love to up the level of the geocaches.  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">title</th>\n",
       "      <th>count</th>\n",
       "      <td>996</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>990</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>I want to learn to speak intellectually.</td>\n",
       "      <td>Login issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "subreddit                                 IWantToLearn  \\\n",
       "text  count                                        890   \n",
       "      unique                                       890   \n",
       "      top         Especially in piano, basketball, etc   \n",
       "      freq                                           1   \n",
       "title count                                        996   \n",
       "      unique                                       990   \n",
       "      top     I want to learn to speak intellectually.   \n",
       "      freq                                           2   \n",
       "\n",
       "subreddit                                            geocaching  \n",
       "text  count                                                 474  \n",
       "      unique                                                474  \n",
       "      top     Would love to up the level of the geocaches.  ...  \n",
       "      freq                                                    1  \n",
       "title count                                                 999  \n",
       "      unique                                                999  \n",
       "      top                                          Login issues  \n",
       "      freq                                                    1  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('subreddit').describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining duplicate Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I want to learn to speak intellectually.                           2\n",
       "IWTL how to sleep on my back                                       2\n",
       "IWTL how to meditate                                               2\n",
       "IWTL how to improve my logical thinking and problem solving.       2\n",
       "I want to learn how to sing                                        2\n",
       "IWTL how to play the piano                                         2\n",
       "IWTL just enough music theory that I could start writing my own    1\n",
       "Name: title, dtype: int64"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].value_counts().sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>My whole life I’ve been sleeping on my front a...</td>\n",
       "      <td>IWTL how to sleep on my back</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>It is really difficult for me to fall asleep w...</td>\n",
       "      <td>IWTL how to sleep on my back</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1220  My whole life I’ve been sleeping on my front a...   \n",
       "1906  It is really difficult for me to fall asleep w...   \n",
       "\n",
       "                             title     subreddit  \n",
       "1220  IWTL how to sleep on my back  IWantToLearn  \n",
       "1906  IWTL how to sleep on my back  IWantToLearn  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['title']=='IWTL how to sleep on my back']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a duplicate, just a similar title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>Any and all advice are welcome. (Iwtl easy ico...</td>\n",
       "      <td>IWTL how to play the piano</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>So I've finally decided I'm going to make use ...</td>\n",
       "      <td>IWTL how to play the piano</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1021  Any and all advice are welcome. (Iwtl easy ico...   \n",
       "1212  So I've finally decided I'm going to make use ...   \n",
       "\n",
       "                           title     subreddit  \n",
       "1021  IWTL how to play the piano  IWantToLearn  \n",
       "1212  IWTL how to play the piano  IWantToLearn  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['title']=='IWTL how to play the piano']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, text body is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>I never had proper training on singing but I k...</td>\n",
       "      <td>I want to learn how to sing</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>Long short story I can't afford a vocal coach....</td>\n",
       "      <td>I want to learn how to sing</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1573  I never had proper training on singing but I k...   \n",
       "1733  Long short story I can't afford a vocal coach....   \n",
       "\n",
       "                            title     subreddit  \n",
       "1573  I want to learn how to sing  IWantToLearn  \n",
       "1733  I want to learn how to sing  IWantToLearn  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['title']=='I want to learn how to sing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, text body is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>I'm trying to learn to meditate but I'm not su...</td>\n",
       "      <td>IWTL how to meditate</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>I want to try it out but I don't know where to...</td>\n",
       "      <td>IWTL how to meditate</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text                 title  \\\n",
       "1284  I'm trying to learn to meditate but I'm not su...  IWTL how to meditate   \n",
       "1403  I want to try it out but I don't know where to...  IWTL how to meditate   \n",
       "\n",
       "         subreddit  \n",
       "1284  IWantToLearn  \n",
       "1403  IWantToLearn  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['title']=='IWTL how to meditate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, text body is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>I am awkwardly uncomfortable speaking out loud...</td>\n",
       "      <td>I want to learn to speak intellectually.</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>I am awkwardly uncomfortable speaking out loud...</td>\n",
       "      <td>I want to learn to speak intellectually.</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1009  I am awkwardly uncomfortable speaking out loud...   \n",
       "1979  I am awkwardly uncomfortable speaking out loud...   \n",
       "\n",
       "                                         title     subreddit  \n",
       "1009  I want to learn to speak intellectually.  IWantToLearn  \n",
       "1979  I want to learn to speak intellectually.  IWantToLearn  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['title']=='I want to learn to speak intellectually.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am awkwardly uncomfortable speaking out loud about anything that I don’t know much about.  I constantly feel like my vocabulary is not where I want it to be.  I’m in college and I feel like I write like an 8th grader.  I say thinks like “like” and “dude” and “that’s what she said” a lot and I can’t seem to stop.  How do I change this?  Before you guys tell me to read more books, what exact books should I be reading?  Any other tips are welcomed.'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[1009]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am awkwardly uncomfortable speaking out loud about anything that I don’t know much about.  I constantly feel like my vocabulary is not where I want it to be.  I’m in college and I feel like I write like an 8th grader.  I say things like “like” and “dude” and “that’s what she said” a lot and I can’t seem to stop.  How do I change this?  Before you guys tell me to read more books, what exact books should I be reading?  Any other tips are welcomed.'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[1979]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['text'].iloc[1009])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['text'].iloc[1979])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "diff = []\n",
    "for char in df['text'].iloc[1009]:\n",
    "    if char not in df['text'].iloc[1979]:\n",
    "        diff.append(char)\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are duplicates even though I ran `.drop_duplicates` I can find no difference between the two posts so I will drop one arbitrarily anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(1979, axis='index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>Hi guys, are there any good courses, games or ...</td>\n",
       "      <td>IWTL how to improve my logical thinking and pr...</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>Hi guys, are there any good courses, games or ...</td>\n",
       "      <td>IWTL how to improve my logical thinking and pr...</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1002  Hi guys, are there any good courses, games or ...   \n",
       "1049  Hi guys, are there any good courses, games or ...   \n",
       "\n",
       "                                                  title     subreddit  \n",
       "1002  IWTL how to improve my logical thinking and pr...  IWantToLearn  \n",
       "1049  IWTL how to improve my logical thinking and pr...  IWantToLearn  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['title']=='IWTL how to improve my logical thinking and problem solving.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi guys, are there any good courses, games or exercises to improve my logical thinking and problem solving ?\\nHow can i improve them ?\\nThanks in Advance.'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[1002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi guys, are there any good courses, games or exercises ti improve mi logical thinking and problem solving ?\\nHow can i improve them ?\\nThanks in Advance.'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[1049]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is almost a verbatum text description but we can see that there were a few typos. What this means is that the redditor posted this thread and then edited it, but the Webscraper picked up both postings. In this case, drop the first post so as not to bias the models on the mytyped words and double count or weight the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(1049, axis='index', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1993, 3)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see that the two duplicates were removed.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the removal of duplicates, the new dataframe is 1993 posts long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boolean masks to examine geocaching and IWantToLearn subreddits separately.\n",
    "geocaching = df['subreddit'] == 'geocaching'\n",
    "iwtl = df['subreddit'] == 'IWantToLearn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         631\n",
       "title          0\n",
       "subreddit      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         525\n",
       "title          0\n",
       "subreddit      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[geocaching].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         106\n",
       "title          0\n",
       "subreddit      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[iwtl].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About half of the geocaching posts do not have text in the post description, while only 10% of IWantToLearn posts do not. What this could mean is that IWantToLearn redditors are more willing to go into detail in their posts while geocaching redditors do not or post links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill empty text posts with 'NA'\n",
    "df.fillna('NA', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining Title and Text by number of words in each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at length of geocaching title\n",
    "geo_title_length = df[geocaching]['title'].map(clean_func)\n",
    "geo_title_length = geo_title_length.str.split()\n",
    "geo_title_length = geo_title_length.apply(len)\n",
    "\n",
    "# Plot as a histogram\n",
    "geo_title_length.plot(bins=50, kind='hist');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    999.000000\n",
       "mean       9.554555\n",
       "std        8.033421\n",
       "min        1.000000\n",
       "25%        4.000000\n",
       "50%        7.000000\n",
       "75%       12.000000\n",
       "max       62.000000\n",
       "Name: title, dtype: float64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_title_length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly distributed under 10 words but really skewed with a long tail. So there are only a few outlier titles that are really long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at length of geocaching text\n",
    "geo_text_length = df[geocaching]['text'].map(clean_func)\n",
    "geo_text_length = geo_text_length.str.split()\n",
    "geo_text_length = geo_text_length.apply(len)\n",
    "\n",
    "geo_text_length.plot(bins=50, kind='hist');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many posts have no text, or very short text bodies. But there is a much more longer skew with the tail on the right. Since text bodies can be longer, there are a few posts that are even longer than the longest titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    999.000000\n",
       "mean      41.844845\n",
       "std       70.957119\n",
       "min        1.000000\n",
       "25%        1.000000\n",
       "50%        1.000000\n",
       "75%       61.000000\n",
       "max      612.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_text_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at length of IWantToLearn title\n",
    "iwtl_title_length = df[iwtl]['title'].map(clean_func)\n",
    "iwtl_title_length = iwtl_title_length.str.split()\n",
    "iwtl_title_length = iwtl_title_length.apply(len)\n",
    "\n",
    "iwtl_title_length.plot(bins=50, kind='hist');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    994.000000\n",
       "mean       8.823944\n",
       "std        5.089154\n",
       "min        1.000000\n",
       "25%        6.000000\n",
       "50%        8.000000\n",
       "75%       10.000000\n",
       "max       61.000000\n",
       "Name: title, dtype: float64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iwtl_title_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at length of IWantToLearn text\n",
    "iwtl_text_length = df[iwtl]['text'].map(clean_func)\n",
    "iwtl_text_length = iwtl_text_length.str.split()\n",
    "iwtl_text_length = iwtl_text_length.apply(len)\n",
    "\n",
    "iwtl_text_length.plot(bins=50, kind='hist');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     994.000000\n",
       "mean       75.691147\n",
       "std        99.553647\n",
       "min         1.000000\n",
       "25%        27.000000\n",
       "50%        54.000000\n",
       "75%        92.000000\n",
       "max      1657.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iwtl_text_length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a glance it looks like IWantToLearn titles and Geocaching titles are mostly distributed in less than 10 words. IWantToLearn posts have fewer \"title only\" type posts and tend to be longer than the geocaching posts that do have text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Corpora\n",
    "\n",
    "Keep title and post text separate before CountVector/TF-IDF and create models from each separately. Later, I will explore whether combining both title and post text make a significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_corpus = df['title']\n",
    "text_corpus = df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since I want to predict a binary variable - subreddit `0` for geocaching and `1` for IWantToLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['subreddit'].map({'geocaching': 0, 'IWantToLearn': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Master stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a master stopword list from both stopword lists\n",
    "custom_stopwords = list(set(stopwords.words('english') + list(stop_words.ENGLISH_STOP_WORDS)))\n",
    "\n",
    "# Add 'na' because it indicates an empty text post\n",
    "custom_stopwords.extend(['na'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stopword list that only takes out 'na'\n",
    "no_na = ['na']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate countvectorizer\n",
    "cvec = CountVectorizer(stop_words=custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_counts = cvec.fit_transform(X_train['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.DataFrame(title_counts.todense(), columns=cvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shine           560\n",
       "looks           143\n",
       "feed            121\n",
       "figure           91\n",
       "fighting         84\n",
       "explorist        71\n",
       "fl               57\n",
       "exports          47\n",
       "added            43\n",
       "collaborated     36\n",
       "calendar         31\n",
       "single           30\n",
       "met              29\n",
       "extra            29\n",
       "binge            28\n",
       "receiving        27\n",
       "etiquette        27\n",
       "automotive       25\n",
       "pay              24\n",
       "boys             24\n",
       "dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what top counted words appear in the Titles\n",
    "top_title_words_c = counts.sum().sort_values(ascending=False)[0:20]\n",
    "top_title_words_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_counts = cvec.fit_transform(X_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts2 = pd.DataFrame(text_counts.todense(), columns=cvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "electro          505\n",
       "advertised       376\n",
       "fucking          365\n",
       "oregon           271\n",
       "gender           262\n",
       "geocachingnsw    253\n",
       "tutorials        250\n",
       "matlab           220\n",
       "fork             186\n",
       "2017             176\n",
       "em               173\n",
       "ol               173\n",
       "haha             154\n",
       "21               148\n",
       "mainly           145\n",
       "mladen           143\n",
       "distractible     141\n",
       "intonation       140\n",
       "openly           137\n",
       "worlds           137\n",
       "dtype: int64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what top counted words appear in the Text post\n",
    "top_text_words_c = counts2.sum().sort_values(ascending=False)[0:20]\n",
    "top_text_words_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note of interest that words that are part of each subreddit's culture do not appear in the top 20 or even top 50 lists. Words such as \"geocache\" and \"iwtl\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1494, 9911)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_counts = pd.concat([counts, counts2], axis=1)\n",
    "all_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency\n",
    "\n",
    "#### Does it make a difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate \n",
    "tvec = TfidfVectorizer(stop_words=custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tf = tvec.fit_transform(X_train['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tf_fit = pd.DataFrame(title_tf.todense(), columns=tvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shine           96.182752\n",
       "looks           39.276772\n",
       "feed            34.215720\n",
       "figure          28.322361\n",
       "fighting        27.203960\n",
       "explorist       24.345841\n",
       "fl              17.475491\n",
       "exports         14.163197\n",
       "added           14.118537\n",
       "collaborated    13.954623\n",
       "met             12.110098\n",
       "extra           12.033046\n",
       "single          11.677608\n",
       "pay             11.657905\n",
       "calendar        10.943987\n",
       "receiving       10.621959\n",
       "etiquette       10.613149\n",
       "binge           10.331351\n",
       "20              10.156783\n",
       "augusta         10.054043\n",
       "dtype: float64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See that there were a few changes to top words when inverse document frequency is applied\n",
    "top_title_words_tf = title_tf_fit.sum().sort_values(ascending=False)[0:20]\n",
    "top_title_words_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tf = tvec.fit_transform(X_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tf_fit = pd.DataFrame(text_tf.todense(), columns=tvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "electro          34.715441\n",
       "advertised       29.653051\n",
       "fucking          29.457742\n",
       "oregon           24.700624\n",
       "tutorials        20.429052\n",
       "gender           20.376762\n",
       "geocachingnsw    18.942764\n",
       "2017             18.252320\n",
       "mladen           17.776934\n",
       "matlab           17.671167\n",
       "ol               17.613669\n",
       "fork             16.223027\n",
       "em               15.973496\n",
       "intonation       15.262113\n",
       "haha             14.821731\n",
       "distractible     13.402571\n",
       "wheel            13.181590\n",
       "openly           13.070319\n",
       "mainly           13.066385\n",
       "21               12.800917\n",
       "dtype: float64"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_text_words_tf = text_tf_fit.sum().sort_values(ascending=False)[0:20]\n",
    "top_text_words_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between CountVectorizer and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>shine</th>\n",
       "      <td>560.0</td>\n",
       "      <td>96.182752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>looks</th>\n",
       "      <td>143.0</td>\n",
       "      <td>39.276772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feed</th>\n",
       "      <td>121.0</td>\n",
       "      <td>34.215720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>figure</th>\n",
       "      <td>91.0</td>\n",
       "      <td>28.322361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fighting</th>\n",
       "      <td>84.0</td>\n",
       "      <td>27.203960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>explorist</th>\n",
       "      <td>71.0</td>\n",
       "      <td>24.345841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fl</th>\n",
       "      <td>57.0</td>\n",
       "      <td>17.475491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exports</th>\n",
       "      <td>47.0</td>\n",
       "      <td>14.163197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>added</th>\n",
       "      <td>43.0</td>\n",
       "      <td>14.118537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collaborated</th>\n",
       "      <td>36.0</td>\n",
       "      <td>13.954623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calendar</th>\n",
       "      <td>31.0</td>\n",
       "      <td>10.943987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single</th>\n",
       "      <td>30.0</td>\n",
       "      <td>11.677608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>met</th>\n",
       "      <td>29.0</td>\n",
       "      <td>12.110098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extra</th>\n",
       "      <td>29.0</td>\n",
       "      <td>12.033046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>binge</th>\n",
       "      <td>28.0</td>\n",
       "      <td>10.331351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>receiving</th>\n",
       "      <td>27.0</td>\n",
       "      <td>10.621959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etiquette</th>\n",
       "      <td>27.0</td>\n",
       "      <td>10.613149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>automotive</th>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pay</th>\n",
       "      <td>24.0</td>\n",
       "      <td>11.657905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boys</th>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.156783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>augusta</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.054043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0          1\n",
       "shine         560.0  96.182752\n",
       "looks         143.0  39.276772\n",
       "feed          121.0  34.215720\n",
       "figure         91.0  28.322361\n",
       "fighting       84.0  27.203960\n",
       "explorist      71.0  24.345841\n",
       "fl             57.0  17.475491\n",
       "exports        47.0  14.163197\n",
       "added          43.0  14.118537\n",
       "collaborated   36.0  13.954623\n",
       "calendar       31.0  10.943987\n",
       "single         30.0  11.677608\n",
       "met            29.0  12.110098\n",
       "extra          29.0  12.033046\n",
       "binge          28.0  10.331351\n",
       "receiving      27.0  10.621959\n",
       "etiquette      27.0  10.613149\n",
       "automotive     25.0        NaN\n",
       "pay            24.0  11.657905\n",
       "boys           24.0        NaN\n",
       "20              NaN  10.156783\n",
       "augusta         NaN  10.054043"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([top_title_words_c, top_title_words_tf]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 10 title words are the same between both vectorizers but the TF-IDF made \"20\" and \"augusta\" of interest while lowering the importance of \"automotive\" and \"boys\". \"met\", \"pay\", and \"extra\" became more of interest than \"calendar\", \"single\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>electro</th>\n",
       "      <td>505.0</td>\n",
       "      <td>34.715441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advertised</th>\n",
       "      <td>376.0</td>\n",
       "      <td>29.653051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fucking</th>\n",
       "      <td>365.0</td>\n",
       "      <td>29.457742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oregon</th>\n",
       "      <td>271.0</td>\n",
       "      <td>24.700624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>262.0</td>\n",
       "      <td>20.376762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geocachingnsw</th>\n",
       "      <td>253.0</td>\n",
       "      <td>18.942764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tutorials</th>\n",
       "      <td>250.0</td>\n",
       "      <td>20.429052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matlab</th>\n",
       "      <td>220.0</td>\n",
       "      <td>17.671167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fork</th>\n",
       "      <td>186.0</td>\n",
       "      <td>16.223027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>176.0</td>\n",
       "      <td>18.252320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>em</th>\n",
       "      <td>173.0</td>\n",
       "      <td>15.973496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ol</th>\n",
       "      <td>173.0</td>\n",
       "      <td>17.613669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>haha</th>\n",
       "      <td>154.0</td>\n",
       "      <td>14.821731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>148.0</td>\n",
       "      <td>12.800917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mainly</th>\n",
       "      <td>145.0</td>\n",
       "      <td>13.066385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mladen</th>\n",
       "      <td>143.0</td>\n",
       "      <td>17.776934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distractible</th>\n",
       "      <td>141.0</td>\n",
       "      <td>13.402571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intonation</th>\n",
       "      <td>140.0</td>\n",
       "      <td>15.262113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openly</th>\n",
       "      <td>137.0</td>\n",
       "      <td>13.070319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worlds</th>\n",
       "      <td>137.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wheel</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13.181590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0          1\n",
       "electro        505.0  34.715441\n",
       "advertised     376.0  29.653051\n",
       "fucking        365.0  29.457742\n",
       "oregon         271.0  24.700624\n",
       "gender         262.0  20.376762\n",
       "geocachingnsw  253.0  18.942764\n",
       "tutorials      250.0  20.429052\n",
       "matlab         220.0  17.671167\n",
       "fork           186.0  16.223027\n",
       "2017           176.0  18.252320\n",
       "em             173.0  15.973496\n",
       "ol             173.0  17.613669\n",
       "haha           154.0  14.821731\n",
       "21             148.0  12.800917\n",
       "mainly         145.0  13.066385\n",
       "mladen         143.0  17.776934\n",
       "distractible   141.0  13.402571\n",
       "intonation     140.0  15.262113\n",
       "openly         137.0  13.070319\n",
       "worlds         137.0        NaN\n",
       "wheel            NaN  13.181590"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([top_text_words_c, top_text_words_tf]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top text words had more movement in terms of importance from CountVectorizer to TF-IDF, but only one new word difference between the two top lists. The top 5 remained the same. \"geocachingnsw\" and \"tutorials\" changed places.\n",
    "\n",
    "For performance reasons, I am choosing to use TF-IDF as a preprocessing vectorizer over CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out different parameters for TF-IDF using GridSearchCV and the MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title TF-IDF\n",
    "\n",
    "Create a pipeline and GridSearch over the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('model', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'tvec__stop_words': ['english', custom_stopwords],\n",
    "    'tvec__analyzer': ['word', preprocess],\n",
    "    'tvec__max_df': [250, 500, 750],\n",
    "    'tvec__min_df': [1, 2, 3],\n",
    "    'tvec__ngram_range': [(1, 1), (1, 3)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tvec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...e,\n",
       "        vocabulary=None)), ('model', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'tvec__stop_words': ['english', ['made', 'with', 'wouldn', 'among', \"hasn't\", 'anywhere', 'already', 'all', \"won't\", 'please', 'won', 'our', 'least', 'amoungst', 'eleven', 'seemed', 'whatever', 'mill', \"you'd\", 'always', 'whereafter', 'those', 'down', 'here', 've', 'itself', 'hereby', \"s..., 'tvec__max_df': [250, 500, 750], 'tvec__min_df': [1, 2, 3], 'tvec__ngram_range': [(1, 1), (1, 3)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(pipeline, param_grid=params, cv=5)\n",
    "gs.fit(X_train['title'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9497991967871486"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best score on titles: 0.950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tvec__analyzer': 'word', 'tvec__max_df': 500, 'tvec__min_df': 4, 'tvec__ngram_range': (1, 1), 'tvec__stop_words': [\"you'd\", 'same', 'o', 'get', 'off', 'you', 'one', 'this', 'even', 'after', 'beforehand', 'seem', 'but', 'once', 'because', \"couldn't\", 'much', 'otherwise', 'on', 'side', 'were', 'had', 'don', 'last', 'whereby', 'interest', 'what', 'along', 'less', 'mightn', 'forty', 'our', 'both', 'except', 'being', 'hasnt', 'aren', 'every', 'why', 'sometimes', 'an', \"wasn't\", 'thin', 'whom', \"didn't\", 'before', 'hereafter', 'then', 'isn', 'indeed', 'keep', 'myself', 'move', 'nine', 'above', 'describe', 'between', 're', 'might', 'besides', 'call', 'to', 'only', 'own', 'without', 'namely', 'herein', 'meanwhile', 'not', 've', 'theirs', 'rather', 'are', 'can', 'again', 'whenever', 'is', 'next', 'others', 'fifteen', 'shan', 'them', 'there', 'inc', 'yourself', 'full', 'do', 'by', 'cant', 'have', 'i', 'over', 'won', 'him', 'further', 'con', 'something', 'very', 'it', 'moreover', 'was', 'doing', \"you've\", 'fifty', 'via', 'several', 'against', 'more', 'thus', 'where', \"mustn't\", \"hadn't\", 'his', 'shouldn', 'the', 'thence', 'down', \"you'll\", 'its', 'as', \"aren't\", 'until', 'weren', 'another', 'll', 'during', 'fill', 'around', 'ours', 'becoming', 'everyone', 'he', 'go', 'three', 'too', 'together', 'never', 'with', 'most', 'or', 'always', 'himself', 'per', 'whereafter', 'y', 'these', 'couldn', 'we', 'seeming', 'least', 'their', 'everything', 'does', 'seemed', 'under', 'sincere', 'whether', 'me', 'became', 'mill', 'whither', 'ma', 'serious', 'been', 'alone', 'done', 'none', 'may', 'co', 'therein', \"haven't\", 'than', 'thereby', \"she's\", 'through', \"needn't\", 'find', 'two', 'ltd', 'eleven', 'here', \"mightn't\", 'hence', 'some', 'out', 'sixty', 'herself', 'will', 'mustn', 'needn', 'due', 'first', 'which', 'who', 'your', 'bottom', 'put', 'no', 'each', \"don't\", 'sometime', 'bill', 'below', \"shouldn't\", \"that'll\", 'latterly', 'six', 'us', 'anyway', 'within', 'other', 'such', 'about', 'please', 'thru', 'nothing', 'when', 'name', 'yours', 'now', 'ain', 'afterwards', \"isn't\", 'd', 'just', 'amoungst', 'ever', 'becomes', 'would', 'she', 'towards', 'either', 'few', 'wasn', 'hadn', 'amongst', 'up', 'empty', 'show', 'elsewhere', 'they', \"should've\", 'many', 'somehow', 'thereafter', 'doesn', 'whoever', 'whereas', 'often', 'm', 'could', 'five', 'ten', 'whence', 'nor', 'has', 'someone', \"weren't\", 'yourselves', 'having', 'see', \"hasn't\", 'anywhere', 'all', 'so', \"wouldn't\", 'nobody', 'haven', 'take', 'behind', 'mine', 'did', 'hereby', \"shan't\", 'mostly', 'neither', 'ourselves', 'nevertheless', 'well', 'any', 'four', 'be', 'her', 'beside', 'formerly', 'front', 'beyond', 'latter', 'and', 'should', 'former', 'that', 'wherein', 'whereupon', 'thick', 'if', 'yet', 'hasn', 'found', 'anyhow', 'am', \"it's\", 'across', 'noone', 'system', \"won't\", 'fire', 'a', 'itself', 'almost', 'hereupon', 'third', 'twelve', 'whole', 'nowhere', 'somewhere', 'wherever', 'although', 'back', 'those', 'how', 'made', \"doesn't\", 'else', 'whatever', 'eight', 'everywhere', 'anything', 'ie', 's', 'etc', 'throughout', 'anyone', 'thereupon', 'for', 'wouldn', 'seems', 'couldnt', 'already', 'twenty', 't', 'must', 'eg', 'from', 'themselves', 'into', 'since', 'still', 'whose', 'cry', 'while', 'though', 'onto', 'toward', 'hundred', 'didn', 'also', 'perhaps', 'in', 'at', 'enough', 'amount', 'of', 'give', 'detail', 'my', 'cannot', 'upon', 'top', 'hers', 'therefore', 'part', 'de', \"you're\", 'become', 'however', 'among', 'un', 'na']}\n"
     ]
    }
   ],
   "source": [
    "# gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters on titles:\n",
    "\n",
    "`{'tvec__analyzer': 'word',\n",
    " 'tvec__max_df': 500,\n",
    " 'tvec__min_df': 1,\n",
    " 'tvec__ngram_range': (1, 3),\n",
    " 'tvec__stop_words': custom_stopwords}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8937875751503006"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test['title'], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a 10% drop in accuracy score from training to testing sets, indicating some amount of overfitting and high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best TF-IDF\n",
    "Using these parameters, fit and transform the title corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tvec = TfidfVectorizer(analyzer='word', max_df=500, min_df=1, ngram_range=(1,3), stop_words=custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the Training Titles\n",
    "best_title_tf = best_tvec.fit_transform(X_train['title'])\n",
    "\n",
    "# Put into a dataframe in order to feed into a model\n",
    "best_title_X_train = pd.DataFrame(best_title_tf.todense(), columns=best_tvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just transform the Testing Titles\n",
    "best_title_tf2 = best_tvec.transform(X_test['title'])\n",
    "\n",
    "# Put into a dataframe to feed into model\n",
    "best_title_X_test = pd.DataFrame(best_title_tf2.todense(), columns=best_tvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text TF-IDF\n",
    "\n",
    "Create a pipeline and GridSearch over the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('model', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'tvec__stop_words': ['english', custom_stopwords],\n",
    "    'tvec__analyzer': ['word'],\n",
    "    'tvec__max_df': [250, 500, 750],\n",
    "    'tvec__min_df': [3, 4, 5],\n",
    "    'tvec__ngram_range': [(1, 1)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tvec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...e,\n",
       "        vocabulary=None)), ('model', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'tvec__stop_words': ['english', ['made', 'with', 'wouldn', 'among', \"hasn't\", 'anywhere', 'already', 'all', \"won't\", 'please', 'won', 'our', 'least', 'amoungst', 'eleven', 'seemed', 'whatever', 'mill', \"you'd\", 'always', 'whereafter', 'those', 'down', 'here', 've', 'itself', 'hereby', \"s...['word'], 'tvec__max_df': [250, 500, 750], 'tvec__min_df': [3, 4, 5], 'tvec__ngram_range': [(1, 1)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(pipeline, param_grid=params, cv=5)\n",
    "gs.fit(X_train['text'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9183400267737617"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best score on text: 0.918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9178356713426854"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test['text'], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters on text:\n",
    "\n",
    "`{'tvec__analyzer': 'word',\n",
    " 'tvec__max_df': 500,\n",
    " 'tvec__min_df': 4,\n",
    " 'tvec__ngram_range': (1, 1),\n",
    " 'tvec__stop_words': 'custom_stopwords'}`\n",
    " \n",
    " By comparison between the two GridSearches, we see that though they differ in max_df, min_df, and ngram range both can agree on the use of the default `'word'` analyzer and the use of 'na' as the only stopword. However, I am still going to use the custom_stopwords to limit the number of word features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best TF-IDF\n",
    "Using these parameters, fit and transform the text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tvec = TfidfVectorizer(analyzer='word', max_df=500, ngram_range=(1,1), min_df= 4, stop_words=custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the Training Text\n",
    "best_text_tf = best_tvec.fit_transform(X_train['text'])\n",
    "\n",
    "# Make into dataframe for modeling\n",
    "best_text_X_train = pd.DataFrame(best_text_tf.todense(), columns=best_tvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the Testing Text\n",
    "best_text_tf2 = best_tvec.transform(X_test['text'])\n",
    "\n",
    "# Make into dataframe for modeling\n",
    "best_text_X_test = pd.DataFrame(best_text_tf2.todense(), columns=best_tvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the vectorized Title and Text tables into one table of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_corpus = pd.concat([best_title_X_train, best_text_X_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1494, 13544)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_corpus = pd.concat([best_title_X_test, best_text_X_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(499, 13544)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_corpus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "04563b69-f7b6-466f-9d65-fc62c9ddee6a"
   },
   "source": [
    "# Predicting subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a7afb2c0-d41e-4779-8216-91cd8dd4473f"
   },
   "source": [
    "#### The baseline accuracy for this model is the percentage of the majority class on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "focus": false,
    "id": "87a17d3d-b7f4-4747-9f75-f9af1d18a174"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geocaching      0.501254\n",
       "IWantToLearn    0.498746\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this means is that we expect the model to have at least an accuracy of 50% to be better than random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "## Random Forests\n",
    "\n",
    "#### Using a `RandomForestClassifier` model to predict which subreddit a given post belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "focus": false,
    "id": "588f9845-6143-4bcc-bfd1-85d45b79303d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf.fit(X_train_corpus, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "focus": false,
    "id": "c20d2498-151c-44c3-a453-3a333c79a0ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9819277108433735"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_train_corpus, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8717434869739479"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test_corpus, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the model defaults, a Random Forest Classifier performs pretty highly. We can see evidence of overfitting in the drop in accuracy from the training set to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.91      0.88       250\n",
      "          1       0.90      0.83      0.87       249\n",
      "\n",
      "avg / total       0.87      0.87      0.87       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Figure out how to display words that were important for classification\n",
    "\n",
    "# rf_coef = pd.DataFrame(rf.feature_importances_, columns=X_train_corpus.columns)\n",
    "# rf_coef.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9367beff-72ba-4768-a0ba-a50b335de61d"
   },
   "source": [
    "# Delete Later\n",
    "#### Use cross-validation in scikit-learn to evaluate the model above. \n",
    "- Evaluate the accuracy of the model, as well as any other metrics you feel are appropriate. \n",
    "- **Bonus**: Use `GridSearchCV` with `Pipeline` to optimize your `CountVectorizer`/`TfidfVectorizer` and classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'random_state': [42],\n",
    "    #'n_estimators': [9, 10, 11],\n",
    "    #'criterion': ['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'random_state': [42]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf2 = GridSearchCV(RandomForestClassifier(), param_grid=rf_params, cv=5)\n",
    "rf2.fit(X_train_corpus, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8688085676037484"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rf2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8717434869739479"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test_corpus, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinb_params = {\n",
    "    'alpha': [1.0,],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "       fit_params=None, iid=True, n_jobs=1, param_grid={'alpha': [1.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinb = GridSearchCV(MultinomialNB(), param_grid=multinb_params, cv=5)\n",
    "multinb.fit(X_train_corpus, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9451137884872824"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = multinb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9619238476953907"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test_corpus, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {\n",
    "    'random_state': [42],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'random_state': [42]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = GridSearchCV(LogisticRegression(), param_grid=params, cv=5)\n",
    "lr.fit(X_train_corpus, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9149933065595717"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.935871743486974"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test_corpus, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Pipeline\n",
    "\n",
    "Picked one ensemble model (Random Forest Classifier), a Naive-Bayes model (Multinomial), and Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter dictionary of all the models and specific tunings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {\n",
    "\n",
    "    # Random Forest\n",
    "    'rf': {\n",
    "        'estimator': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'rf__random_state': [42],\n",
    "            #'rf__n_estimators': [9, 10, 11],\n",
    "            #'rf__max_depth': [],\n",
    "            #'rf__critereon': ['gini', 'entropy']\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # Logistic Regression\n",
    "    'lr': {\n",
    "        'estimator': LogisticRegression(),\n",
    "        'params': {\n",
    "            'lr__random_state': [42],\n",
    "            #'lr__C': [1.0],\n",
    "            #'lr__': []\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # Multinomial Naive-Bayes\n",
    "    'multinb': {\n",
    "        'estimator': MultinomialNB(),\n",
    "        'params': {\n",
    "            #'multinb__alpha': [1.0]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GrideSearch for Estimator  rf\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.1s remaining:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    3.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done fitting:  rf\n",
      "--------------------\n",
      "Running GrideSearch for Estimator  lr\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    3.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done fitting:  lr\n",
      "--------------------\n",
      "Running GrideSearch for Estimator  multinb\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.8s remaining:    2.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done fitting:  multinb\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    3.1s finished\n"
     ]
    }
   ],
   "source": [
    "# Make an empty dictionary of fitted models \n",
    "fitted_models = {}\n",
    "\n",
    "# Loop through models in pipelines, tuning each one and saving it to fitted_models\n",
    "for step, config in estimators.items():\n",
    "    pipe = Pipeline(\n",
    "        steps = [\n",
    "            (step, config['estimator'])\n",
    "        ]\n",
    "    )\n",
    "    # Create cross-validation object\n",
    "    model = GridSearchCV(pipe, param_grid=config['params'], cv=5, n_jobs=-1, verbose=1)\n",
    "    print('Running GrideSearch for Estimator ', step)\n",
    "    \n",
    "    \n",
    "    fitted_models[step] = model.fit(X_train_corpus, y_train), model.best_score_\n",
    "    print('Done fitting: ', step)\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-5c0606987702>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfitted_models\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfitted_models\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "for name in fitted_models.items():\n",
    "    print(name[0], name[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf': (GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=Pipeline(memory=None,\n",
       "       steps=[('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "              oob_score=False, random_state=None, verbose=0,\n",
       "              warm_start=False))]),\n",
       "         fit_params=None, iid=True, n_jobs=-1,\n",
       "         param_grid={'rf__random_state': [42]}, pre_dispatch='2*n_jobs',\n",
       "         refit=True, return_train_score='warn', scoring=None, verbose=1),\n",
       "  0.8688085676037484),\n",
       " 'lr': (GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=Pipeline(memory=None,\n",
       "       steps=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False))]),\n",
       "         fit_params=None, iid=True, n_jobs=-1,\n",
       "         param_grid={'lr__random_state': [42]}, pre_dispatch='2*n_jobs',\n",
       "         refit=True, return_train_score='warn', scoring=None, verbose=1),\n",
       "  0.9149933065595717),\n",
       " 'multinb': (GridSearchCV(cv=5, error_score='raise',\n",
       "         estimator=Pipeline(memory=None,\n",
       "       steps=[('multinb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "         fit_params=None, iid=True, n_jobs=-1, param_grid={},\n",
       "         pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "         scoring=None, verbose=1), 0.9451137884872824)}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea: Have separate branch where combine Title and Post Text then run count vectorizer / TF-IDF here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['title'] + ' ' + df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('model', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'tvec__stop_words': ['english', custom_stopwords],\n",
    "    'tvec__analyzer': ['word', preprocess],\n",
    "    'tvec__max_df': [250, 500, 750],\n",
    "    'tvec__min_df': [1, 2, 3],\n",
    "    'tvec__ngram_range': [(1, 1), (1, 3)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tvec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...e,\n",
       "        vocabulary=None)), ('model', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'tvec__stop_words': ['english', ['hundred', 'nowhere', \"doesn't\", 'across', 't', 'my', 'm', 'we', 'wherever', 'name', 'her', 've', \"that'll\", 'give', 'about', 'ourselves', \"shouldn't\", 'made', 'via', 'found', 'so', 'etc', 'it', 'by', 'show', 'your', 'on', 'move', 'the', 'us', 'below', 'd..., 'tvec__max_df': [250, 500, 750], 'tvec__min_df': [1, 2, 3], 'tvec__ngram_range': [(1, 1), (1, 3)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(pipeline, param_grid=params, cv=5)\n",
    "gs.fit(X_train['title'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9497991967871486"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a difference in learning and outdoor exploration?\n",
    "\n",
    "Both involve curiosity and wandering around in odd places, both on the internet and in the real world."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
