{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Using Reddit's API for Classifying Subreddit Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook regards the classification of geocaching and IWantToLearn subreddit posts from Reddit using Natural Language Processing techniques. For the workflow of obtaining the posts using webscraping techniques, please refer to the Jupyter Notebook \"Project 3 Webscraping.ipynb\". This project was segmented into two to prevent from re-running webscraping code.\n",
    "\n",
    "\n",
    "\n",
    "Problem statement: What characteristics of a post on Reddit contribute most to what subreddit it belongs to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning/handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# NLP specific libraries\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "import re\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Modeling Libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring in Dataset of Posts scraped from Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./reddit_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IWantToLearn    1000\n",
       "geocaching      1000\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are an even 1000 posts scraped from Reddit from our Webscraping efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using regular expressions to remove punctuation in a function\n",
    "def no_punct(string):\n",
    "    return re.sub(\"[.,üòØ?üòä!‚Äô\\\";^+`:*'()-@‚Äù‚Äú=>_$&<~%|{}\\[\\]]\", \" \", string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to clean any column or to feed into a word vectorizer as an analyzer parameter\n",
    "def clean_func(column):\n",
    "    \n",
    "    #remove puntuation with punctuation removal function\n",
    "    column = no_punct(column)\n",
    "    \n",
    "    #lowercase\n",
    "    column = column.lower()\n",
    "    \n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that utilizes lemmatizing and a general Regex to remove punctuation\n",
    "\n",
    "def preprocess(text):\n",
    "    # instantiate lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # words only Regex, removes punctuation\n",
    "    text = re.sub(\"[^A-Za-z]\", \" \", text)\n",
    "    \n",
    "    # lemmatize\n",
    "    text = lemmatizer.lemmatize(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1995, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>1364</td>\n",
       "      <td>1364</td>\n",
       "      <td>For a guy I'm rather emotional, I want to get ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>1995</td>\n",
       "      <td>1989</td>\n",
       "      <td>IWTL how to improve my logical thinking and pr...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <td>1995</td>\n",
       "      <td>2</td>\n",
       "      <td>geocaching</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count unique                                                top freq\n",
       "text       1364   1364  For a guy I'm rather emotional, I want to get ...    1\n",
       "title      1995   1989  IWTL how to improve my logical thinking and pr...    2\n",
       "subreddit  1995      2                                         geocaching  999"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>IWantToLearn</th>\n",
       "      <th>geocaching</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">text</th>\n",
       "      <th>count</th>\n",
       "      <td>890</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>890</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>I don't think I'm really particularly shy or a...</td>\n",
       "      <td>I was going  for the World Turtle souvenir yes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">title</th>\n",
       "      <th>count</th>\n",
       "      <td>996</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>990</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>I want to learn how to sing</td>\n",
       "      <td>Trying to get started</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "subreddit                                          IWantToLearn  \\\n",
       "text  count                                                 890   \n",
       "      unique                                                890   \n",
       "      top     I don't think I'm really particularly shy or a...   \n",
       "      freq                                                    1   \n",
       "title count                                                 996   \n",
       "      unique                                                990   \n",
       "      top                           I want to learn how to sing   \n",
       "      freq                                                    2   \n",
       "\n",
       "subreddit                                            geocaching  \n",
       "text  count                                                 474  \n",
       "      unique                                                474  \n",
       "      top     I was going  for the World Turtle souvenir yes...  \n",
       "      freq                                                    1  \n",
       "title count                                                 999  \n",
       "      unique                                                999  \n",
       "      top                                 Trying to get started  \n",
       "      freq                                                    1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('subreddit').describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining duplicate Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IWTL how to play the piano                                      2\n",
       "IWTL how to improve my logical thinking and problem solving.    2\n",
       "IWTL how to meditate                                            2\n",
       "I want to learn to speak intellectually.                        2\n",
       "I want to learn how to sing                                     2\n",
       "IWTL how to sleep on my back                                    2\n",
       "IWTL how to be honest without being a complete dick             1\n",
       "Sneaky Decoy Camera Geocache!                                   1\n",
       "What phone do you use?                                          1\n",
       "IWTL estimating project cost and design fast.                   1\n",
       "Name: title, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title'].value_counts().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>My whole life I‚Äôve been sleeping on my front a...</td>\n",
       "      <td>IWTL how to sleep on my back</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>It is really difficult for me to fall asleep w...</td>\n",
       "      <td>IWTL how to sleep on my back</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1220  My whole life I‚Äôve been sleeping on my front a...   \n",
       "1906  It is really difficult for me to fall asleep w...   \n",
       "\n",
       "                             title     subreddit  \n",
       "1220  IWTL how to sleep on my back  IWantToLearn  \n",
       "1906  IWTL how to sleep on my back  IWantToLearn  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['title']=='IWTL how to sleep on my back']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a duplicate, just a similar title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>Any and all advice are welcome. (Iwtl easy ico...</td>\n",
       "      <td>IWTL how to play the piano</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>So I've finally decided I'm going to make use ...</td>\n",
       "      <td>IWTL how to play the piano</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1021  Any and all advice are welcome. (Iwtl easy ico...   \n",
       "1212  So I've finally decided I'm going to make use ...   \n",
       "\n",
       "                           title     subreddit  \n",
       "1021  IWTL how to play the piano  IWantToLearn  \n",
       "1212  IWTL how to play the piano  IWantToLearn  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['title']=='IWTL how to play the piano']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, text body is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>I never had proper training on singing but I k...</td>\n",
       "      <td>I want to learn how to sing</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>Long short story I can't afford a vocal coach....</td>\n",
       "      <td>I want to learn how to sing</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1573  I never had proper training on singing but I k...   \n",
       "1733  Long short story I can't afford a vocal coach....   \n",
       "\n",
       "                            title     subreddit  \n",
       "1573  I want to learn how to sing  IWantToLearn  \n",
       "1733  I want to learn how to sing  IWantToLearn  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['title']=='I want to learn how to sing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, text body is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>I'm trying to learn to meditate but I'm not su...</td>\n",
       "      <td>IWTL how to meditate</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>I want to try it out but I don't know where to...</td>\n",
       "      <td>IWTL how to meditate</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text                 title  \\\n",
       "1284  I'm trying to learn to meditate but I'm not su...  IWTL how to meditate   \n",
       "1403  I want to try it out but I don't know where to...  IWTL how to meditate   \n",
       "\n",
       "         subreddit  \n",
       "1284  IWantToLearn  \n",
       "1403  IWantToLearn  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['title']=='IWTL how to meditate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, text body is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>I am awkwardly uncomfortable speaking out loud...</td>\n",
       "      <td>I want to learn to speak intellectually.</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>I am awkwardly uncomfortable speaking out loud...</td>\n",
       "      <td>I want to learn to speak intellectually.</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1009  I am awkwardly uncomfortable speaking out loud...   \n",
       "1979  I am awkwardly uncomfortable speaking out loud...   \n",
       "\n",
       "                                         title     subreddit  \n",
       "1009  I want to learn to speak intellectually.  IWantToLearn  \n",
       "1979  I want to learn to speak intellectually.  IWantToLearn  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['title']=='I want to learn to speak intellectually.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am awkwardly uncomfortable speaking out loud about anything that I don‚Äôt know much about.  I constantly feel like my vocabulary is not where I want it to be.  I‚Äôm in college and I feel like I write like an 8th grader.  I say thinks like ‚Äúlike‚Äù and ‚Äúdude‚Äù and ‚Äúthat‚Äôs what she said‚Äù a lot and I can‚Äôt seem to stop.  How do I change this?  Before you guys tell me to read more books, what exact books should I be reading?  Any other tips are welcomed.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[1009]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am awkwardly uncomfortable speaking out loud about anything that I don‚Äôt know much about.  I constantly feel like my vocabulary is not where I want it to be.  I‚Äôm in college and I feel like I write like an 8th grader.  I say things like ‚Äúlike‚Äù and ‚Äúdude‚Äù and ‚Äúthat‚Äôs what she said‚Äù a lot and I can‚Äôt seem to stop.  How do I change this?  Before you guys tell me to read more books, what exact books should I be reading?  Any other tips are welcomed.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[1979]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['text'].iloc[1009])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "451"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['text'].iloc[1979])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "diff = []\n",
    "for char in df['text'].iloc[1009]:\n",
    "    if char not in df['text'].iloc[1979]:\n",
    "        diff.append(char)\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are duplicates even though I ran `.drop_duplicates` I can find no difference between the two posts so I will drop one arbitrarily anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(1979, axis='index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>Hi guys, are there any good courses, games or ...</td>\n",
       "      <td>IWTL how to improve my logical thinking and pr...</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>Hi guys, are there any good courses, games or ...</td>\n",
       "      <td>IWTL how to improve my logical thinking and pr...</td>\n",
       "      <td>IWantToLearn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "1002  Hi guys, are there any good courses, games or ...   \n",
       "1049  Hi guys, are there any good courses, games or ...   \n",
       "\n",
       "                                                  title     subreddit  \n",
       "1002  IWTL how to improve my logical thinking and pr...  IWantToLearn  \n",
       "1049  IWTL how to improve my logical thinking and pr...  IWantToLearn  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['title']=='IWTL how to improve my logical thinking and problem solving.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi guys, are there any good courses, games or exercises to improve my logical thinking and problem solving ?\\nHow can i improve them ?\\nThanks in Advance.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[1002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi guys, are there any good courses, games or exercises ti improve mi logical thinking and problem solving ?\\nHow can i improve them ?\\nThanks in Advance.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[1049]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is almost a verbatum text description but we can see that there were a few typos. What this means is that the redditor posted this thread and then edited it, but the Webscraper picked up both postings. In this case, drop the first post so as not to bias the models on the mytyped words and double count or weight the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(1049, axis='index', inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1993, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see that the two duplicates were removed.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the removal of duplicates, the new dataframe is 1993 posts long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boolean masks to examine geocaching and IWantToLearn subreddits separately.\n",
    "geocaching = df['subreddit'] == 'geocaching'\n",
    "iwtl = df['subreddit'] == 'IWantToLearn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         631\n",
       "title          0\n",
       "subreddit      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         525\n",
       "title          0\n",
       "subreddit      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[geocaching].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         106\n",
       "title          0\n",
       "subreddit      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[iwtl].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About half of the geocaching posts do not have text in the post description, while only 10% of IWantToLearn posts do not. What this could mean is that IWantToLearn redditors are more willing to go into detail in their posts while geocaching redditors do not or post links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill empty text posts with 'NA'\n",
    "df.fillna('NA', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining Title and Text by number of words in each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at length of geocaching title\n",
    "geo_title_length = df[geocaching]['title'].map(clean_func)\n",
    "geo_title_length = geo_title_length.str.split()\n",
    "geo_title_length = geo_title_length.apply(len)\n",
    "\n",
    "# Plot as a histogram\n",
    "geo_title_length.plot(bins=50, kind='hist');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    999.000000\n",
       "mean       9.554555\n",
       "std        8.033421\n",
       "min        1.000000\n",
       "25%        4.000000\n",
       "50%        7.000000\n",
       "75%       12.000000\n",
       "max       62.000000\n",
       "Name: title, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_title_length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly distributed under 10 words but really skewed with a long tail. So there are only a few outlier titles that are really long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at length of geocaching text\n",
    "geo_text_length = df[geocaching]['text'].map(clean_func)\n",
    "geo_text_length = geo_text_length.str.split()\n",
    "geo_text_length = geo_text_length.apply(len)\n",
    "\n",
    "geo_text_length.plot(bins=50, kind='hist');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many posts have no text, or very short text bodies. But there is a much more longer skew with the tail on the right. Since text bodies can be longer, there are a few posts that are even longer than the longest titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    999.000000\n",
       "mean      41.844845\n",
       "std       70.957119\n",
       "min        1.000000\n",
       "25%        1.000000\n",
       "50%        1.000000\n",
       "75%       61.000000\n",
       "max      612.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_text_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at length of IWantToLearn title\n",
    "iwtl_title_length = df[iwtl]['title'].map(clean_func)\n",
    "iwtl_title_length = iwtl_title_length.str.split()\n",
    "iwtl_title_length = iwtl_title_length.apply(len)\n",
    "\n",
    "iwtl_title_length.plot(bins=50, kind='hist');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    994.000000\n",
       "mean       8.823944\n",
       "std        5.089154\n",
       "min        1.000000\n",
       "25%        6.000000\n",
       "50%        8.000000\n",
       "75%       10.000000\n",
       "max       61.000000\n",
       "Name: title, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iwtl_title_length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at length of IWantToLearn text\n",
    "iwtl_text_length = df[iwtl]['text'].map(clean_func)\n",
    "iwtl_text_length = iwtl_text_length.str.split()\n",
    "iwtl_text_length = iwtl_text_length.apply(len)\n",
    "\n",
    "iwtl_text_length.plot(bins=50, kind='hist');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     994.000000\n",
       "mean       75.691147\n",
       "std        99.553647\n",
       "min         1.000000\n",
       "25%        27.000000\n",
       "50%        54.000000\n",
       "75%        92.000000\n",
       "max      1657.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iwtl_text_length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a glance it looks like IWantToLearn titles and Geocaching titles are mostly distributed in less than 10 words. IWantToLearn posts have fewer \"title only\" type posts and tend to be longer than the geocaching posts that do have text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Corpora\n",
    "\n",
    "Keep title and post text separate before CountVector/TF-IDF and create models from each separately. Later, I will explore whether combining both title and post text make a significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_corpus = df['title']\n",
    "text_corpus = df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since I want to predict a binary variable - subreddit `0` for geocaching and `1` for IWantToLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['subreddit'].map({'geocaching': 0, 'IWantToLearn': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "#### Use `CountVectorizer` or `TfidfVectorizer` from scikit-learn to create features from the thread titles and descriptions (NOTE: Not all threads have a description)\n",
    "- Examine using count or binary features in the model\n",
    "- Re-evaluate your models using these. Does this improve the model performance? \n",
    "- What text features are the most valuable? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Master stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a master stopword list from both stopword lists\n",
    "custom_stopwords = list(set(stopwords.words('english') + list(stop_words.ENGLISH_STOP_WORDS)))\n",
    "\n",
    "# Add 'na' because it indicates an empty text post\n",
    "custom_stopwords.extend(['na'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stopword list that only takes out 'na'\n",
    "no_na = ['na']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate countvectorizer\n",
    "cvec = CountVectorizer(stop_words=custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_counts = cvec.fit_transform(X_train['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.DataFrame(title_counts.todense(), columns=cvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shine           560\n",
       "looks           143\n",
       "feed            121\n",
       "figure           91\n",
       "fighting         84\n",
       "explorist        71\n",
       "fl               57\n",
       "exports          47\n",
       "added            43\n",
       "collaborated     36\n",
       "calendar         31\n",
       "single           30\n",
       "met              29\n",
       "extra            29\n",
       "binge            28\n",
       "receiving        27\n",
       "etiquette        27\n",
       "automotive       25\n",
       "pay              24\n",
       "boys             24\n",
       "dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what top counted words appear in the Titles\n",
    "top_title_words_c = counts.sum().sort_values(ascending=False)[0:20]\n",
    "top_title_words_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_counts = cvec.fit_transform(X_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts2 = pd.DataFrame(text_counts.todense(), columns=cvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "electro          505\n",
       "advertised       376\n",
       "fucking          365\n",
       "oregon           271\n",
       "gender           262\n",
       "geocachingnsw    253\n",
       "tutorials        250\n",
       "matlab           220\n",
       "fork             186\n",
       "2017             176\n",
       "em               173\n",
       "ol               173\n",
       "haha             154\n",
       "21               148\n",
       "mainly           145\n",
       "mladen           143\n",
       "distractible     141\n",
       "intonation       140\n",
       "openly           137\n",
       "worlds           137\n",
       "dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what top counted words appear in the Text post\n",
    "top_text_words_c = counts2.sum().sort_values(ascending=False)[0:20]\n",
    "top_text_words_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note of interest that words that are part of each subreddit's culture do not appear in the top 20 or even top 50 lists. Words such as \"geocache\" and \"iwtl\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency\n",
    "\n",
    "#### Does it make a difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate \n",
    "tvec = TfidfVectorizer(stop_words=custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tf = tvec.fit_transform(X_train['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tf_fit = pd.DataFrame(title_tf.todense(), columns=tvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shine           96.182752\n",
       "looks           39.276772\n",
       "feed            34.215720\n",
       "figure          28.322361\n",
       "fighting        27.203960\n",
       "explorist       24.345841\n",
       "fl              17.475491\n",
       "exports         14.163197\n",
       "added           14.118537\n",
       "collaborated    13.954623\n",
       "met             12.110098\n",
       "extra           12.033046\n",
       "single          11.677608\n",
       "pay             11.657905\n",
       "calendar        10.943987\n",
       "receiving       10.621959\n",
       "etiquette       10.613149\n",
       "binge           10.331351\n",
       "20              10.156783\n",
       "augusta         10.054043\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See that there were a few changes to top words when inverse document frequency is applied\n",
    "top_title_words_tf = title_tf_fit.sum().sort_values(ascending=False)[0:20]\n",
    "top_title_words_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tf = tvec.fit_transform(X_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tf_fit = pd.DataFrame(text_tf.todense(), columns=tvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "electro          34.715441\n",
       "advertised       29.653051\n",
       "fucking          29.457742\n",
       "oregon           24.700624\n",
       "tutorials        20.429052\n",
       "gender           20.376762\n",
       "geocachingnsw    18.942764\n",
       "2017             18.252320\n",
       "mladen           17.776934\n",
       "matlab           17.671167\n",
       "ol               17.613669\n",
       "fork             16.223027\n",
       "em               15.973496\n",
       "intonation       15.262113\n",
       "haha             14.821731\n",
       "distractible     13.402571\n",
       "wheel            13.181590\n",
       "openly           13.070319\n",
       "mainly           13.066385\n",
       "21               12.800917\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_text_words_tf = text_tf_fit.sum().sort_values(ascending=False)[0:20]\n",
    "top_text_words_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between CountVectorizer and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>shine</th>\n",
       "      <td>560.0</td>\n",
       "      <td>96.182752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>looks</th>\n",
       "      <td>143.0</td>\n",
       "      <td>39.276772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feed</th>\n",
       "      <td>121.0</td>\n",
       "      <td>34.215720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>figure</th>\n",
       "      <td>91.0</td>\n",
       "      <td>28.322361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fighting</th>\n",
       "      <td>84.0</td>\n",
       "      <td>27.203960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>explorist</th>\n",
       "      <td>71.0</td>\n",
       "      <td>24.345841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fl</th>\n",
       "      <td>57.0</td>\n",
       "      <td>17.475491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exports</th>\n",
       "      <td>47.0</td>\n",
       "      <td>14.163197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>added</th>\n",
       "      <td>43.0</td>\n",
       "      <td>14.118537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collaborated</th>\n",
       "      <td>36.0</td>\n",
       "      <td>13.954623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calendar</th>\n",
       "      <td>31.0</td>\n",
       "      <td>10.943987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single</th>\n",
       "      <td>30.0</td>\n",
       "      <td>11.677608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>met</th>\n",
       "      <td>29.0</td>\n",
       "      <td>12.110098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extra</th>\n",
       "      <td>29.0</td>\n",
       "      <td>12.033046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>binge</th>\n",
       "      <td>28.0</td>\n",
       "      <td>10.331351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>receiving</th>\n",
       "      <td>27.0</td>\n",
       "      <td>10.621959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>etiquette</th>\n",
       "      <td>27.0</td>\n",
       "      <td>10.613149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>automotive</th>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pay</th>\n",
       "      <td>24.0</td>\n",
       "      <td>11.657905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boys</th>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.156783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>augusta</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10.054043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0          1\n",
       "shine         560.0  96.182752\n",
       "looks         143.0  39.276772\n",
       "feed          121.0  34.215720\n",
       "figure         91.0  28.322361\n",
       "fighting       84.0  27.203960\n",
       "explorist      71.0  24.345841\n",
       "fl             57.0  17.475491\n",
       "exports        47.0  14.163197\n",
       "added          43.0  14.118537\n",
       "collaborated   36.0  13.954623\n",
       "calendar       31.0  10.943987\n",
       "single         30.0  11.677608\n",
       "met            29.0  12.110098\n",
       "extra          29.0  12.033046\n",
       "binge          28.0  10.331351\n",
       "receiving      27.0  10.621959\n",
       "etiquette      27.0  10.613149\n",
       "automotive     25.0        NaN\n",
       "pay            24.0  11.657905\n",
       "boys           24.0        NaN\n",
       "20              NaN  10.156783\n",
       "augusta         NaN  10.054043"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([top_title_words_c, top_title_words_tf]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 10 title words are the same between both vectorizers but the TF-IDF made \"20\" and \"augusta\" of interest while lowering the importance of \"automotive\" and \"boys\". \"met\", \"pay\", and \"extra\" became more of interest than \"calendar\", \"single\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>electro</th>\n",
       "      <td>505.0</td>\n",
       "      <td>34.715441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advertised</th>\n",
       "      <td>376.0</td>\n",
       "      <td>29.653051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fucking</th>\n",
       "      <td>365.0</td>\n",
       "      <td>29.457742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oregon</th>\n",
       "      <td>271.0</td>\n",
       "      <td>24.700624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>262.0</td>\n",
       "      <td>20.376762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>geocachingnsw</th>\n",
       "      <td>253.0</td>\n",
       "      <td>18.942764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tutorials</th>\n",
       "      <td>250.0</td>\n",
       "      <td>20.429052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matlab</th>\n",
       "      <td>220.0</td>\n",
       "      <td>17.671167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fork</th>\n",
       "      <td>186.0</td>\n",
       "      <td>16.223027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>176.0</td>\n",
       "      <td>18.252320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>em</th>\n",
       "      <td>173.0</td>\n",
       "      <td>15.973496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ol</th>\n",
       "      <td>173.0</td>\n",
       "      <td>17.613669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>haha</th>\n",
       "      <td>154.0</td>\n",
       "      <td>14.821731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>148.0</td>\n",
       "      <td>12.800917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mainly</th>\n",
       "      <td>145.0</td>\n",
       "      <td>13.066385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mladen</th>\n",
       "      <td>143.0</td>\n",
       "      <td>17.776934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distractible</th>\n",
       "      <td>141.0</td>\n",
       "      <td>13.402571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intonation</th>\n",
       "      <td>140.0</td>\n",
       "      <td>15.262113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openly</th>\n",
       "      <td>137.0</td>\n",
       "      <td>13.070319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worlds</th>\n",
       "      <td>137.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wheel</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13.181590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0          1\n",
       "electro        505.0  34.715441\n",
       "advertised     376.0  29.653051\n",
       "fucking        365.0  29.457742\n",
       "oregon         271.0  24.700624\n",
       "gender         262.0  20.376762\n",
       "geocachingnsw  253.0  18.942764\n",
       "tutorials      250.0  20.429052\n",
       "matlab         220.0  17.671167\n",
       "fork           186.0  16.223027\n",
       "2017           176.0  18.252320\n",
       "em             173.0  15.973496\n",
       "ol             173.0  17.613669\n",
       "haha           154.0  14.821731\n",
       "21             148.0  12.800917\n",
       "mainly         145.0  13.066385\n",
       "mladen         143.0  17.776934\n",
       "distractible   141.0  13.402571\n",
       "intonation     140.0  15.262113\n",
       "openly         137.0  13.070319\n",
       "worlds         137.0        NaN\n",
       "wheel            NaN  13.181590"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([top_text_words_c, top_text_words_tf]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top text words had more movement in terms of importance from CountVectorizer to TF-IDF, but only one new word difference between the two top lists. The top 5 remained the same. \"geocachingnsw\" and \"tutorials\" changed places.\n",
    "\n",
    "For performance reasons, I am choosing to use TF-IDF as a preprocessing vectorizer over CountVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out different parameters for TF-IDF using GridSearchCV and the MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commenting out after having run the following code to prevent it from running from an accidental \"Run All\" command and hanging all computer processing power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('model', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'tvec__stop_words': ['english', custom_stopwords],\n",
    "    'tvec__analyzer': ['word', preprocess],\n",
    "    'tvec__max_df': [250, 500, 750],\n",
    "    'tvec__min_df': [1, 2, 3],\n",
    "    'tvec__ngram_range': [(1, 1), (1, 3)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tvec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...e,\n",
       "        vocabulary=None)), ('model', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'tvec__stop_words': ['english', ['hundred', 'nowhere', \"doesn't\", 'across', 't', 'my', 'm', 'we', 'wherever', 'name', 'her', 've', \"that'll\", 'give', 'about', 'ourselves', \"shouldn't\", 'made', 'via', 'found', 'so', 'etc', 'it', 'by', 'show', 'your', 'on', 'move', 'the', 'us', 'below', 'd..., 'tvec__max_df': [250, 500, 750], 'tvec__min_df': [1, 2, 3], 'tvec__ngram_range': [(1, 1), (1, 3)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(pipeline, param_grid=params, cv=5)\n",
    "gs.fit(X_train['title'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9497991967871486"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best score on titles: 0.950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tvec__analyzer': 'word',\n",
       " 'tvec__max_df': 500,\n",
       " 'tvec__min_df': 1,\n",
       " 'tvec__ngram_range': (1, 3),\n",
       " 'tvec__stop_words': ['hundred',\n",
       "  'nowhere',\n",
       "  \"doesn't\",\n",
       "  'across',\n",
       "  't',\n",
       "  'my',\n",
       "  'm',\n",
       "  'we',\n",
       "  'wherever',\n",
       "  'name',\n",
       "  'her',\n",
       "  've',\n",
       "  \"that'll\",\n",
       "  'give',\n",
       "  'about',\n",
       "  'ourselves',\n",
       "  \"shouldn't\",\n",
       "  'made',\n",
       "  'via',\n",
       "  'found',\n",
       "  'so',\n",
       "  'etc',\n",
       "  'it',\n",
       "  'by',\n",
       "  'show',\n",
       "  'your',\n",
       "  'on',\n",
       "  'move',\n",
       "  'the',\n",
       "  'us',\n",
       "  'below',\n",
       "  'd',\n",
       "  'himself',\n",
       "  'detail',\n",
       "  'one',\n",
       "  'didn',\n",
       "  'further',\n",
       "  'neither',\n",
       "  'go',\n",
       "  'none',\n",
       "  'forty',\n",
       "  'here',\n",
       "  'after',\n",
       "  'even',\n",
       "  'alone',\n",
       "  'often',\n",
       "  'same',\n",
       "  'at',\n",
       "  'shan',\n",
       "  'a',\n",
       "  'very',\n",
       "  'weren',\n",
       "  'five',\n",
       "  'anyway',\n",
       "  'cry',\n",
       "  'ltd',\n",
       "  'otherwise',\n",
       "  'where',\n",
       "  'too',\n",
       "  'couldnt',\n",
       "  'i',\n",
       "  'whereafter',\n",
       "  'seem',\n",
       "  'throughout',\n",
       "  'sixty',\n",
       "  'herself',\n",
       "  'its',\n",
       "  'own',\n",
       "  'together',\n",
       "  'take',\n",
       "  'three',\n",
       "  \"hasn't\",\n",
       "  \"didn't\",\n",
       "  'from',\n",
       "  'never',\n",
       "  'yourself',\n",
       "  'are',\n",
       "  'most',\n",
       "  'thin',\n",
       "  'was',\n",
       "  'still',\n",
       "  'fill',\n",
       "  'will',\n",
       "  'nor',\n",
       "  'though',\n",
       "  'find',\n",
       "  'four',\n",
       "  'when',\n",
       "  'our',\n",
       "  'seemed',\n",
       "  'aren',\n",
       "  'there',\n",
       "  'every',\n",
       "  \"mightn't\",\n",
       "  'meanwhile',\n",
       "  'therein',\n",
       "  'somehow',\n",
       "  'someone',\n",
       "  'sometime',\n",
       "  'mill',\n",
       "  \"don't\",\n",
       "  \"wasn't\",\n",
       "  'becomes',\n",
       "  'any',\n",
       "  'six',\n",
       "  'therefore',\n",
       "  'keep',\n",
       "  'again',\n",
       "  'thence',\n",
       "  'his',\n",
       "  'beside',\n",
       "  'but',\n",
       "  'each',\n",
       "  'among',\n",
       "  'now',\n",
       "  'just',\n",
       "  'all',\n",
       "  'put',\n",
       "  'nothing',\n",
       "  'few',\n",
       "  'much',\n",
       "  'off',\n",
       "  'become',\n",
       "  'un',\n",
       "  'out',\n",
       "  'as',\n",
       "  'behind',\n",
       "  \"needn't\",\n",
       "  \"she's\",\n",
       "  'above',\n",
       "  'such',\n",
       "  'bottom',\n",
       "  'twelve',\n",
       "  'least',\n",
       "  'were',\n",
       "  'does',\n",
       "  'former',\n",
       "  'into',\n",
       "  \"should've\",\n",
       "  'him',\n",
       "  'already',\n",
       "  'back',\n",
       "  \"won't\",\n",
       "  'anywhere',\n",
       "  'ever',\n",
       "  'before',\n",
       "  'seems',\n",
       "  'their',\n",
       "  'beforehand',\n",
       "  'nevertheless',\n",
       "  \"mustn't\",\n",
       "  'am',\n",
       "  'between',\n",
       "  'indeed',\n",
       "  'many',\n",
       "  'which',\n",
       "  'nobody',\n",
       "  'don',\n",
       "  'first',\n",
       "  'done',\n",
       "  'while',\n",
       "  'onto',\n",
       "  'shouldn',\n",
       "  'front',\n",
       "  'herein',\n",
       "  'having',\n",
       "  'latter',\n",
       "  'yet',\n",
       "  'of',\n",
       "  'along',\n",
       "  'nine',\n",
       "  'namely',\n",
       "  'anyhow',\n",
       "  'another',\n",
       "  'y',\n",
       "  're',\n",
       "  \"weren't\",\n",
       "  'has',\n",
       "  'both',\n",
       "  'thereby',\n",
       "  'll',\n",
       "  'whether',\n",
       "  'always',\n",
       "  'other',\n",
       "  'within',\n",
       "  'to',\n",
       "  'formerly',\n",
       "  'system',\n",
       "  'two',\n",
       "  's',\n",
       "  'over',\n",
       "  'mostly',\n",
       "  'third',\n",
       "  'sincere',\n",
       "  'or',\n",
       "  'whereby',\n",
       "  \"haven't\",\n",
       "  'not',\n",
       "  'eg',\n",
       "  'anyone',\n",
       "  'mightn',\n",
       "  'you',\n",
       "  'through',\n",
       "  'had',\n",
       "  'whither',\n",
       "  'might',\n",
       "  'thus',\n",
       "  'she',\n",
       "  'around',\n",
       "  'could',\n",
       "  'also',\n",
       "  'whose',\n",
       "  'themselves',\n",
       "  'towards',\n",
       "  'describe',\n",
       "  'hadn',\n",
       "  'wherein',\n",
       "  'inc',\n",
       "  'until',\n",
       "  'either',\n",
       "  'thru',\n",
       "  'must',\n",
       "  'itself',\n",
       "  'last',\n",
       "  'couldn',\n",
       "  'me',\n",
       "  'fifteen',\n",
       "  'against',\n",
       "  'afterwards',\n",
       "  'because',\n",
       "  'somewhere',\n",
       "  'fifty',\n",
       "  'seeming',\n",
       "  'rather',\n",
       "  'eleven',\n",
       "  'should',\n",
       "  \"you'll\",\n",
       "  'please',\n",
       "  'hereby',\n",
       "  'perhaps',\n",
       "  'then',\n",
       "  'ma',\n",
       "  'several',\n",
       "  'for',\n",
       "  'o',\n",
       "  'hereupon',\n",
       "  'doesn',\n",
       "  \"aren't\",\n",
       "  \"you've\",\n",
       "  \"couldn't\",\n",
       "  'hers',\n",
       "  'sometimes',\n",
       "  'toward',\n",
       "  'although',\n",
       "  'if',\n",
       "  'whatever',\n",
       "  'empty',\n",
       "  'and',\n",
       "  'whoever',\n",
       "  'hence',\n",
       "  'whole',\n",
       "  'whereupon',\n",
       "  'except',\n",
       "  'part',\n",
       "  'those',\n",
       "  'an',\n",
       "  'since',\n",
       "  'cannot',\n",
       "  'whereas',\n",
       "  'during',\n",
       "  'what',\n",
       "  'some',\n",
       "  'mine',\n",
       "  'than',\n",
       "  'without',\n",
       "  'twenty',\n",
       "  'doing',\n",
       "  'everyone',\n",
       "  'due',\n",
       "  'wouldn',\n",
       "  'well',\n",
       "  'cant',\n",
       "  'upon',\n",
       "  'something',\n",
       "  'per',\n",
       "  'ie',\n",
       "  'wasn',\n",
       "  'became',\n",
       "  'won',\n",
       "  'noone',\n",
       "  'he',\n",
       "  'this',\n",
       "  'others',\n",
       "  'would',\n",
       "  'besides',\n",
       "  'almost',\n",
       "  \"wouldn't\",\n",
       "  'no',\n",
       "  'once',\n",
       "  'why',\n",
       "  'call',\n",
       "  'amount',\n",
       "  'everything',\n",
       "  'everywhere',\n",
       "  \"you're\",\n",
       "  'thereafter',\n",
       "  'amoungst',\n",
       "  'been',\n",
       "  'latterly',\n",
       "  'they',\n",
       "  'moreover',\n",
       "  'side',\n",
       "  'them',\n",
       "  \"you'd\",\n",
       "  'can',\n",
       "  'thereupon',\n",
       "  'whenever',\n",
       "  'needn',\n",
       "  \"it's\",\n",
       "  'fire',\n",
       "  'anything',\n",
       "  'less',\n",
       "  'eight',\n",
       "  'enough',\n",
       "  'haven',\n",
       "  'de',\n",
       "  'is',\n",
       "  'did',\n",
       "  'in',\n",
       "  'hasn',\n",
       "  'get',\n",
       "  'yourselves',\n",
       "  'bill',\n",
       "  'may',\n",
       "  'down',\n",
       "  'more',\n",
       "  'under',\n",
       "  'mustn',\n",
       "  'amongst',\n",
       "  'however',\n",
       "  'becoming',\n",
       "  'be',\n",
       "  \"isn't\",\n",
       "  'theirs',\n",
       "  'ain',\n",
       "  'whom',\n",
       "  'that',\n",
       "  'do',\n",
       "  'have',\n",
       "  'top',\n",
       "  \"hadn't\",\n",
       "  'co',\n",
       "  'yours',\n",
       "  'ours',\n",
       "  'interest',\n",
       "  'ten',\n",
       "  'else',\n",
       "  'next',\n",
       "  'thick',\n",
       "  'hasnt',\n",
       "  'how',\n",
       "  'full',\n",
       "  'con',\n",
       "  'elsewhere',\n",
       "  'hereafter',\n",
       "  'only',\n",
       "  'up',\n",
       "  'with',\n",
       "  'these',\n",
       "  \"shan't\",\n",
       "  'beyond',\n",
       "  'isn',\n",
       "  'who',\n",
       "  'serious',\n",
       "  'whence',\n",
       "  'being',\n",
       "  'see',\n",
       "  'myself',\n",
       "  'na']}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters on titles:\n",
    "\n",
    "`{'tvec__analyzer': 'word',\n",
    " 'tvec__max_df': 500,\n",
    " 'tvec__min_df': 1,\n",
    " 'tvec__ngram_range': (1, 3),\n",
    " 'tvec__stop_words': custom_stopwords}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8937875751503006"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test['title'], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a 10% drop in accuracy score from training to testing sets, indicating some amount of overfitting and high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best TF-IDF\n",
    "Using these parameters, fit and transform the title corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tvec = TfidfVectorizer(analyzer='word', max_df=500, min_df=1, ngram_range=(1,3), stop_words=custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_title_tf = best_tvec.fit_transform(X_train['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_title_tf_fit = pd.DataFrame(best_title_tf.todense(), columns=best_tvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('model', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'tvec__stop_words': ['english', custom_stopwords],\n",
    "    'tvec__analyzer': ['word'],\n",
    "    'tvec__max_df': [250, 500, 750],\n",
    "    'tvec__min_df': [3, 4, 5],\n",
    "    'tvec__ngram_range': [(1, 1)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tvec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...e,\n",
       "        vocabulary=None)), ('model', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'tvec__stop_words': ['english', ['hundred', 'nowhere', \"doesn't\", 'across', 't', 'my', 'm', 'we', 'wherever', 'name', 'her', 've', \"that'll\", 'give', 'about', 'ourselves', \"shouldn't\", 'made', 'via', 'found', 'so', 'etc', 'it', 'by', 'show', 'your', 'on', 'move', 'the', 'us', 'below', 'd...['word'], 'tvec__max_df': [250, 500, 750], 'tvec__min_df': [3, 4, 5], 'tvec__ngram_range': [(1, 1)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(pipeline, param_grid=params, cv=5)\n",
    "gs.fit(X_train['text'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9183400267737617"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best score on text: 0.918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tvec__analyzer': 'word',\n",
       " 'tvec__max_df': 500,\n",
       " 'tvec__min_df': 4,\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__stop_words': ['hundred',\n",
       "  'nowhere',\n",
       "  \"doesn't\",\n",
       "  'across',\n",
       "  't',\n",
       "  'my',\n",
       "  'm',\n",
       "  'we',\n",
       "  'wherever',\n",
       "  'name',\n",
       "  'her',\n",
       "  've',\n",
       "  \"that'll\",\n",
       "  'give',\n",
       "  'about',\n",
       "  'ourselves',\n",
       "  \"shouldn't\",\n",
       "  'made',\n",
       "  'via',\n",
       "  'found',\n",
       "  'so',\n",
       "  'etc',\n",
       "  'it',\n",
       "  'by',\n",
       "  'show',\n",
       "  'your',\n",
       "  'on',\n",
       "  'move',\n",
       "  'the',\n",
       "  'us',\n",
       "  'below',\n",
       "  'd',\n",
       "  'himself',\n",
       "  'detail',\n",
       "  'one',\n",
       "  'didn',\n",
       "  'further',\n",
       "  'neither',\n",
       "  'go',\n",
       "  'none',\n",
       "  'forty',\n",
       "  'here',\n",
       "  'after',\n",
       "  'even',\n",
       "  'alone',\n",
       "  'often',\n",
       "  'same',\n",
       "  'at',\n",
       "  'shan',\n",
       "  'a',\n",
       "  'very',\n",
       "  'weren',\n",
       "  'five',\n",
       "  'anyway',\n",
       "  'cry',\n",
       "  'ltd',\n",
       "  'otherwise',\n",
       "  'where',\n",
       "  'too',\n",
       "  'couldnt',\n",
       "  'i',\n",
       "  'whereafter',\n",
       "  'seem',\n",
       "  'throughout',\n",
       "  'sixty',\n",
       "  'herself',\n",
       "  'its',\n",
       "  'own',\n",
       "  'together',\n",
       "  'take',\n",
       "  'three',\n",
       "  \"hasn't\",\n",
       "  \"didn't\",\n",
       "  'from',\n",
       "  'never',\n",
       "  'yourself',\n",
       "  'are',\n",
       "  'most',\n",
       "  'thin',\n",
       "  'was',\n",
       "  'still',\n",
       "  'fill',\n",
       "  'will',\n",
       "  'nor',\n",
       "  'though',\n",
       "  'find',\n",
       "  'four',\n",
       "  'when',\n",
       "  'our',\n",
       "  'seemed',\n",
       "  'aren',\n",
       "  'there',\n",
       "  'every',\n",
       "  \"mightn't\",\n",
       "  'meanwhile',\n",
       "  'therein',\n",
       "  'somehow',\n",
       "  'someone',\n",
       "  'sometime',\n",
       "  'mill',\n",
       "  \"don't\",\n",
       "  \"wasn't\",\n",
       "  'becomes',\n",
       "  'any',\n",
       "  'six',\n",
       "  'therefore',\n",
       "  'keep',\n",
       "  'again',\n",
       "  'thence',\n",
       "  'his',\n",
       "  'beside',\n",
       "  'but',\n",
       "  'each',\n",
       "  'among',\n",
       "  'now',\n",
       "  'just',\n",
       "  'all',\n",
       "  'put',\n",
       "  'nothing',\n",
       "  'few',\n",
       "  'much',\n",
       "  'off',\n",
       "  'become',\n",
       "  'un',\n",
       "  'out',\n",
       "  'as',\n",
       "  'behind',\n",
       "  \"needn't\",\n",
       "  \"she's\",\n",
       "  'above',\n",
       "  'such',\n",
       "  'bottom',\n",
       "  'twelve',\n",
       "  'least',\n",
       "  'were',\n",
       "  'does',\n",
       "  'former',\n",
       "  'into',\n",
       "  \"should've\",\n",
       "  'him',\n",
       "  'already',\n",
       "  'back',\n",
       "  \"won't\",\n",
       "  'anywhere',\n",
       "  'ever',\n",
       "  'before',\n",
       "  'seems',\n",
       "  'their',\n",
       "  'beforehand',\n",
       "  'nevertheless',\n",
       "  \"mustn't\",\n",
       "  'am',\n",
       "  'between',\n",
       "  'indeed',\n",
       "  'many',\n",
       "  'which',\n",
       "  'nobody',\n",
       "  'don',\n",
       "  'first',\n",
       "  'done',\n",
       "  'while',\n",
       "  'onto',\n",
       "  'shouldn',\n",
       "  'front',\n",
       "  'herein',\n",
       "  'having',\n",
       "  'latter',\n",
       "  'yet',\n",
       "  'of',\n",
       "  'along',\n",
       "  'nine',\n",
       "  'namely',\n",
       "  'anyhow',\n",
       "  'another',\n",
       "  'y',\n",
       "  're',\n",
       "  \"weren't\",\n",
       "  'has',\n",
       "  'both',\n",
       "  'thereby',\n",
       "  'll',\n",
       "  'whether',\n",
       "  'always',\n",
       "  'other',\n",
       "  'within',\n",
       "  'to',\n",
       "  'formerly',\n",
       "  'system',\n",
       "  'two',\n",
       "  's',\n",
       "  'over',\n",
       "  'mostly',\n",
       "  'third',\n",
       "  'sincere',\n",
       "  'or',\n",
       "  'whereby',\n",
       "  \"haven't\",\n",
       "  'not',\n",
       "  'eg',\n",
       "  'anyone',\n",
       "  'mightn',\n",
       "  'you',\n",
       "  'through',\n",
       "  'had',\n",
       "  'whither',\n",
       "  'might',\n",
       "  'thus',\n",
       "  'she',\n",
       "  'around',\n",
       "  'could',\n",
       "  'also',\n",
       "  'whose',\n",
       "  'themselves',\n",
       "  'towards',\n",
       "  'describe',\n",
       "  'hadn',\n",
       "  'wherein',\n",
       "  'inc',\n",
       "  'until',\n",
       "  'either',\n",
       "  'thru',\n",
       "  'must',\n",
       "  'itself',\n",
       "  'last',\n",
       "  'couldn',\n",
       "  'me',\n",
       "  'fifteen',\n",
       "  'against',\n",
       "  'afterwards',\n",
       "  'because',\n",
       "  'somewhere',\n",
       "  'fifty',\n",
       "  'seeming',\n",
       "  'rather',\n",
       "  'eleven',\n",
       "  'should',\n",
       "  \"you'll\",\n",
       "  'please',\n",
       "  'hereby',\n",
       "  'perhaps',\n",
       "  'then',\n",
       "  'ma',\n",
       "  'several',\n",
       "  'for',\n",
       "  'o',\n",
       "  'hereupon',\n",
       "  'doesn',\n",
       "  \"aren't\",\n",
       "  \"you've\",\n",
       "  \"couldn't\",\n",
       "  'hers',\n",
       "  'sometimes',\n",
       "  'toward',\n",
       "  'although',\n",
       "  'if',\n",
       "  'whatever',\n",
       "  'empty',\n",
       "  'and',\n",
       "  'whoever',\n",
       "  'hence',\n",
       "  'whole',\n",
       "  'whereupon',\n",
       "  'except',\n",
       "  'part',\n",
       "  'those',\n",
       "  'an',\n",
       "  'since',\n",
       "  'cannot',\n",
       "  'whereas',\n",
       "  'during',\n",
       "  'what',\n",
       "  'some',\n",
       "  'mine',\n",
       "  'than',\n",
       "  'without',\n",
       "  'twenty',\n",
       "  'doing',\n",
       "  'everyone',\n",
       "  'due',\n",
       "  'wouldn',\n",
       "  'well',\n",
       "  'cant',\n",
       "  'upon',\n",
       "  'something',\n",
       "  'per',\n",
       "  'ie',\n",
       "  'wasn',\n",
       "  'became',\n",
       "  'won',\n",
       "  'noone',\n",
       "  'he',\n",
       "  'this',\n",
       "  'others',\n",
       "  'would',\n",
       "  'besides',\n",
       "  'almost',\n",
       "  \"wouldn't\",\n",
       "  'no',\n",
       "  'once',\n",
       "  'why',\n",
       "  'call',\n",
       "  'amount',\n",
       "  'everything',\n",
       "  'everywhere',\n",
       "  \"you're\",\n",
       "  'thereafter',\n",
       "  'amoungst',\n",
       "  'been',\n",
       "  'latterly',\n",
       "  'they',\n",
       "  'moreover',\n",
       "  'side',\n",
       "  'them',\n",
       "  \"you'd\",\n",
       "  'can',\n",
       "  'thereupon',\n",
       "  'whenever',\n",
       "  'needn',\n",
       "  \"it's\",\n",
       "  'fire',\n",
       "  'anything',\n",
       "  'less',\n",
       "  'eight',\n",
       "  'enough',\n",
       "  'haven',\n",
       "  'de',\n",
       "  'is',\n",
       "  'did',\n",
       "  'in',\n",
       "  'hasn',\n",
       "  'get',\n",
       "  'yourselves',\n",
       "  'bill',\n",
       "  'may',\n",
       "  'down',\n",
       "  'more',\n",
       "  'under',\n",
       "  'mustn',\n",
       "  'amongst',\n",
       "  'however',\n",
       "  'becoming',\n",
       "  'be',\n",
       "  \"isn't\",\n",
       "  'theirs',\n",
       "  'ain',\n",
       "  'whom',\n",
       "  'that',\n",
       "  'do',\n",
       "  'have',\n",
       "  'top',\n",
       "  \"hadn't\",\n",
       "  'co',\n",
       "  'yours',\n",
       "  'ours',\n",
       "  'interest',\n",
       "  'ten',\n",
       "  'else',\n",
       "  'next',\n",
       "  'thick',\n",
       "  'hasnt',\n",
       "  'how',\n",
       "  'full',\n",
       "  'con',\n",
       "  'elsewhere',\n",
       "  'hereafter',\n",
       "  'only',\n",
       "  'up',\n",
       "  'with',\n",
       "  'these',\n",
       "  \"shan't\",\n",
       "  'beyond',\n",
       "  'isn',\n",
       "  'who',\n",
       "  'serious',\n",
       "  'whence',\n",
       "  'being',\n",
       "  'see',\n",
       "  'myself',\n",
       "  'na']}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9178356713426854"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test['text'], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters on text:\n",
    "\n",
    "`{'tvec__analyzer': 'word',\n",
    " 'tvec__max_df': 500,\n",
    " 'tvec__min_df': 4,\n",
    " 'tvec__ngram_range': (1, 1),\n",
    " 'tvec__stop_words': 'custom_stopwords'}`\n",
    " \n",
    " By comparison between the two GridSearches, we see that though they differ in max_df, min_df, and ngram range both can agree on the use of the default `'word'` analyzer and the use of 'na' as the only stopword. However, I am still going to use the custom_stopwords to limit the number of word features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best TF-IDF\n",
    "Using these parameters, fit and transform the text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tvec = TfidfVectorizer(analyzer='word', max_df=500, ngram_range=(1,1), min_df= 4, stop_words=custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_text_tf = best_tvec.fit_transform(X_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_text_tf_fit = pd.DataFrame(best_text_tf.todense(), columns=best_tvec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the vectorized Title and Text tables into one table of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tf_corpus = pd.concat([best_title_tf_fit, best_text_tf_fit], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1993, 18006)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_tf_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "04563b69-f7b6-466f-9d65-fc62c9ddee6a"
   },
   "source": [
    "# Predicting subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a7afb2c0-d41e-4779-8216-91cd8dd4473f"
   },
   "source": [
    "#### The baseline accuracy for this model is the percentage of the majority class on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "focus": false,
    "id": "87a17d3d-b7f4-4747-9f75-f9af1d18a174"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geocaching      0.501254\n",
       "IWantToLearn    0.498746\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this means is that we expect the model to have at least an accuracy of 50% to be better than random guessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "## Random Forests\n",
    "\n",
    "#### Using a `RandomForestClassifier` model to predict which subreddit a given post belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "focus": false,
    "id": "588f9845-6143-4bcc-bfd1-85d45b79303d"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: \"ITWL How to make friends at college, when I'm disabled and socially impaired\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-475696d343fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    245\u001b[0m         \"\"\"\n\u001b[0;32m    246\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    431\u001b[0m                                       force_all_finite)\n\u001b[0;32m    432\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: \"ITWL How to make friends at college, when I'm disabled and socially impaired\""
     ]
    }
   ],
   "source": [
    "# Instantiate\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train['title'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "focus": false,
    "id": "c20d2498-151c-44c3-a453-3a333c79a0ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999330655957162"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_train['title'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9639278557114228"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test['title'], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the model defaults, a Random Forest Classifier performs pretty highly. We can see evidence of overfitting in the drop in accuracy from the training set to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., ..., 0., 0., 0.])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[rf.feature_importances_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.97      0.96       250\n",
      "          1       0.97      0.96      0.96       249\n",
      "\n",
      "avg / total       0.96      0.96      0.96       499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-397f6618c910>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrf_coef\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_tf_corpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mrf_coef\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rf' is not defined"
     ]
    }
   ],
   "source": [
    "rf_coef = pd.DataFrame(rf.feature_importances_.T, columns=best_tf_corpus.columns)\n",
    "rf_coef.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9367beff-72ba-4768-a0ba-a50b335de61d"
   },
   "source": [
    "# Delete Later\n",
    "#### Use cross-validation in scikit-learn to evaluate the model above. \n",
    "- Evaluate the accuracy of the model, as well as any other metrics you feel are appropriate. \n",
    "- **Bonus**: Use `GridSearchCV` with `Pipeline` to optimize your `CountVectorizer`/`TfidfVectorizer` and classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of labels=1195 does not match number of samples=3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-328b61820bed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    326\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 328\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 790\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    791\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             raise ValueError(\"Number of labels=%d does not match \"\n\u001b[1;32m--> 236\u001b[1;33m                              \"number of samples=%d\" % (len(y), n_samples))\n\u001b[0m\u001b[0;32m    237\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_weight_fraction_leaf\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"min_weight_fraction_leaf must in [0, 0.5]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of labels=1195 does not match number of samples=3"
     ]
    }
   ],
   "source": [
    "gs = GridSearchCV(pipeline, param_grid=params, cv=5)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = Pipeline([\n",
    "    ('tfidf', ##best_tfidf),\n",
    "    ('multi', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipeline, param_grid=params, cv=5)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = Pipeline([\n",
    "    ('tfidf', ##best_tfidf)\n",
    "    ('log_reg', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipeline, param_grid=params, cv=5)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'rf': make_pipeline(TfidfVectorizer(), RandomForestClassifier()),\n",
    "    'lr': make_pipeline(TfidfVectorizer(), LogisticRegression()),\n",
    "    'multi': make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter dictionary of all the specific tunings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tfidf_hyperparameters = {\n",
    "    'tf'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Random Forest\n",
    "rf_hyperparameters = {\n",
    "    'rf__n_estimators': [],\n",
    "    'rf__max_depth': [],\n",
    "}\n",
    "\n",
    "# Logistic Regression\n",
    "lr_hyperparameters = {\n",
    "    'lr__': [],\n",
    "    \n",
    "}\n",
    "\n",
    "# Multinomial Naive-Bayes\n",
    "multi_hyperparameters = {\n",
    "    'multi__': [],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary linking named models to respective hyperparameter dictionary\n",
    "hyperparameters = {\n",
    "    'rf': rf_hyperparameters,\n",
    "    'log_reg': lr_hyperparameters,\n",
    "    'multi': multi_hyperparameters, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an empty dictionary of fitted models \n",
    "fitted_models = {}\n",
    "# Loop through models in pipelines, tuning each one and saving it to fitted_models\n",
    "\n",
    "for name , pipeline in pipelines.items():\n",
    "\n",
    "    # Create cross-validation object\n",
    "    model = GridSearchCV(pipeline , hyperparameters[name], cv=5, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    fitted_models[name] = model\n",
    "    print('Done fitting: ' name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in fitted_models.items():\n",
    "    print(name, model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea: Have separate branch where combine Title and Post Text then run count vectorizer / TF-IDF here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['title'] + ' ' + df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('model', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'tvec__stop_words': ['english', custom_stopwords],\n",
    "    'tvec__analyzer': ['word', preprocess],\n",
    "    'tvec__max_df': [250, 500, 750],\n",
    "    'tvec__min_df': [1, 2, 3],\n",
    "    'tvec__ngram_range': [(1, 1), (1, 3)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tvec', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "  ...e,\n",
       "        vocabulary=None)), ('model', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'tvec__stop_words': ['english', ['hundred', 'nowhere', \"doesn't\", 'across', 't', 'my', 'm', 'we', 'wherever', 'name', 'her', 've', \"that'll\", 'give', 'about', 'ourselves', \"shouldn't\", 'made', 'via', 'found', 'so', 'etc', 'it', 'by', 'show', 'your', 'on', 'move', 'the', 'us', 'below', 'd..., 'tvec__max_df': [250, 500, 750], 'tvec__min_df': [1, 2, 3], 'tvec__ngram_range': [(1, 1), (1, 3)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(pipeline, param_grid=params, cv=5)\n",
    "gs.fit(X_train['title'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9497991967871486"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a difference in learning and outdoor exploration?\n",
    "\n",
    "Both involve curiosity and wandering around in odd places, both on the internet and in the real world."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
